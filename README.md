![GitHub repo size](https://img.shields.io/github/repo-size/iuricode/README-template?style=for-the-badge)
![GitHub forks](https://img.shields.io/github/forks/iuricode/README-template?style=for-the-badge)

# Teste Engenheiro de Dados S√™nior

> Projeto que utiliza de uma base de dados dispon√≠vel na web para criar arquivos .parquet, delta live tables, mergear dados entre arquivos e responder quest√µes pontuais.

### Tarefas

O projeto o prjeto foi desenvolvido no decorrer de um dia √∫til concluindo tarefas como:

- [x] Tarefa 1 - Conectar com base de dados Ecommerce type dispon√≠vel em https://uibakery.io/sql-playground ;
- [x] Tarefa 2 - Desenhar diagrama ER do banco;
- [x] Tarefa 3 - Criar notebook no Databricks Community;
- [x] Tarefa 4 - Criar l√≥gica no notebook para leitura de cada tabela da base de dados e salvar cada uma tabela como .parquet;
- [x] Tarefa 5 - Criar merge entre arquivos da base de dados e arquivos .parquet contendo l√≥gica de insert, update e delete para o mesmo, atualizando a delta table;
- [x] Tarefa 6 - Responder respectivas quest√µes:
  Qual pa√≠s possui a maior quantidade de itens cancelados?
  Qual o faturamento da linha de produto mais vendido, considerando como itens "Shipped", cujo o pedido foi realizado no ano de 2005?
  Informar nome, sobrenome e email dos vendedores do Jap√£o com local-part do email mascarado.
- [x] Tarefa 7 - Salvar resultados em Delta Live Table;
- [x] Tarefa 8 - Armazenar artefatos em reposit√≥rio Github p√∫blico;
- [x] Tarefa 9 - Documentar no README.md todo o processo realizado.

## üíª Pr√©-requisitos

Antes de come√ßar, verifique se voc√™ atendeu aos seguintes requisitos:

* Voc√™ realizou o cadastro no Databricks Community no link (https://community.cloud.databricks.com).
* Voc√™ realizou conex√£o com a base de dados ecommerce type em (https://uibakery.io/sql-playground).

## üöÄ Iniciando Teste Engenheiro de Dados S√™nior

# M√≥dulos e frameworks:
  reposit√≥rio: pyspark.sql
  m√≥dulos: SparkSession, DataFrame, functions

  reposit√≥rio: delta.tables
  m√≥dulos: all

  reposit√≥rio: pyspark.sql.utils
  m√≥dulos: AnalyssisException

# Conex√£o com base de dados:
Ap√≥s informadas credenciais de acesso √† base de dados Ecommece type, √© criada uma sparksession utilizando as ent√£o credenciais.

# Declara√ß√£o de fun√ß√µes √∫teis:
Declarada fun√ß√£o "lerTabela" de leitura de tabela da base de dados JDBC pedindo que seja apenas informado nome da tabela para leitura da mesma.

# Leitura de tabelas e Cria√ß√£o de arquivos .parquet
Fun√ß√£o "lerTabela" utilizada iterando pela lista "tabelas" onde possu√≠mos o nome de todas as tabelas at√© ent√£o listadas na base de dados.

Iterando por cada item da lista "tabelas", cria-se um arquivo .parquet para cada respectiva tabela, salvos no diret√≥rio "dbfs:/ecommerce_db/" onde cada leitura do arquivo sobrescreve o arquivo .parquet no diret√≥rio, n√£o mantendo hist√≥rico destes.

# Cria√ß√£o Delta Tables a partir de arquivos .parquet
Criados arquivos .parquet no reposit√≥rio "ecommerce_db", itera-se arquivo a arquivo criando dataframe no formato 'parquet' e identificando as colunas de cada .parquet no dicion√°rio "columnas" onde cada chave refere-se a uma tabela e seus valores cont√©m uma lista com as respectivas colunas.

Consultando exist√™ncia de cada delta table, cria-se uma nova delta table em caso de inexist√™ncia de tabela com o nome 'delta_{arquivo da tabela iterado}'

# Merge
A etapa de merge de dados conta com uma nova itera√ß√£o por cada arquivo parquet contido no diret√≥rio "ecommerce_db".
O dicion√°rio primaryKey conta com chaves contendo nome das delta tables e valores contendo suas respectivas chaves prim√°rias de tabela para identificador para merge.

Por fim, esta itera√ß√£o carrega cada arquivo .parquet em um dataframe Spark e em seguida realiza o merge dos dados contidos no dataframe para a respectiva delta table utilizando a funcionalidade de merge em delta lake.

- Registros contigos no .parquet e n√£o existentes na delta table s√£o inseridos;
- Registros existentes tanto no .parquet quanto delta table s√£o atualizados na delta table de acordo com arquivo .parquet;
- Registros ausentes do arquivo.parquet s√£o removidos da delta table em quest√£o.

# Respostas quest√µes Tarefa 6:
Quest√µes respondidas em modelo de consulta SQL informando a resposta para cada uma.

## ü§ù Colaboradores
Alecsander Santos de Araujo
https://www.linkedin.com/in/santtosdinho/
